{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shouv\\Anaconda3\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries in python\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k,auc_score,reciprocal_rank\n",
    "import scipy\n",
    "import time\n",
    "import math\n",
    "from lightfm.data import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "\n",
    "users = pd.read_csv('users_tag.csv',index_col=0)\n",
    "business = pd.read_csv('business_cj.csv',index_col=0)\n",
    "review = pd.read_csv('IL_review.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For recommender system you want to see which user bought which item or which user rated which item.\n",
    "#In practical scenario users do not rate/ buy every item, large number of users are concentrated on few items \n",
    "# and hence a certain amount of items are untouched by users.\n",
    "\n",
    "# Since we do not have any action of customers on some items while we have it on certain other items, \n",
    "# this emptiness of interaction is called sparsity problem.\n",
    "\n",
    "n_users = review.user_id.unique().shape[0]\n",
    "n_items = review.business_id.unique().shape[0]\n",
    "\n",
    "print('Number of users: {}'.format(n_users))\n",
    "print('Number of models: {}'.format(n_items))\n",
    "print('Sparsity: {:4.3f}%'.format(float(review.shape[0]) / float(n_users*n_items) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are duplicate reviews\n",
    "a = review.groupby(['business_id','user_id']).agg(['count']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1081, 2)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing users with more than one review\n",
    "\n",
    "tmp = a.useful.sort_values(by = 'count',ascending = False).reset_index()\n",
    "tmp[tmp['count'] >1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1081 users have multiple review for one business "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_reviewers = tmp[tmp['count']>1]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       26917\n",
       "1        5843\n",
       "2        5233\n",
       "3       22282\n",
       "4          69\n",
       "        ...  \n",
       "1076    16741\n",
       "1077     5210\n",
       "1078    17853\n",
       "1079     5204\n",
       "1080    27569\n",
       "Name: index, Length: 1081, dtype: int64"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicate entries and taking latest entries of users for a particular business\n",
    "for ind in multiple_reviewers:\n",
    "    tmp = review[(review.user_id == a.loc[ind, 'user_id'][0]) & \n",
    "                 (review.business_id == a.loc[ind, 'business_id'][0])].sort_values(by = 'date', ascending = True)\n",
    "    review.drop(tmp.index[0:len(tmp.index)-1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if duplicate values have been removed\n",
    "b = review.groupby(['business_id','user_id']).agg(['count']).reset_index()\n",
    "tmp = b.useful.sort_values(by = 'count',ascending = False).reset_index()\n",
    "tmp[tmp['count'] >1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the review_stars contains bias, thus needs to be normalized by subtracting the average stars from it and \n",
    "# make negative stars to -1 and positive stars to +1\n",
    "\n",
    "user_ind = review.user_id.unique()\n",
    "for ind in user_ind:\n",
    "    this_avg=users.average_stars[users.user_id == ind]\n",
    "    temp = review.stars[review.user_id == ind] - float(this_avg)\n",
    "    temp[temp>0] = 1\n",
    "    temp[temp<0] = -1\n",
    "    review.stars[review.user_id == ind] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  0.])"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.stars.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num users: 12859, num_items 845.\n"
     ]
    }
   ],
   "source": [
    "# The first thing we need to do is to create a mapping between the user id and business ids from our input data \n",
    "# This mapping will provide indices that will be used internally by our model.\n",
    "\n",
    "# model establishment\n",
    "dataset = Dataset()\n",
    "dataset.fit(review.user_id,review.business_id)\n",
    "num_users, num_items = dataset.interactions_shape()\n",
    "print('Num users: {}, num_items {}.'.format(num_users, num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit item and user features.\n",
    "dataset.fit_partial(items=business.business_id,\n",
    "                    item_features=['stars'])\n",
    "dataset.fit_partial(items=business.business_id,\n",
    "                    item_features=['review_count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Ethnic Food, Food Trucks, Specialty Food, Impo...\n",
       "1      Restaurants, Diners, Sandwiches, Breakfast & B...\n",
       "2                        Hot Dogs, Restaurants, Barbeque\n",
       "3                             Donuts, Food, Coffee & Tea\n",
       "4                                     Pizza, Restaurants\n",
       "                             ...                        \n",
       "840            Sushi Bars, Chinese, Buffets, Restaurants\n",
       "841      Fast Food, Restaurants, Tex-Mex, Mexican, Tacos\n",
       "842                                 Italian, Restaurants\n",
       "843                 Fast Food, Restaurants, Pizza, Salad\n",
       "844                   Sandwiches, Fast Food, Restaurants\n",
       "Name: category, Length: 845, dtype: object"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.category #=business.drop(columns='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Fast Food', 'Nightlife', 'American (Traditional)', 'Bars', 'Pizza',\n",
       "       'Sandwiches', 'Burgers', 'Coffee & Tea', 'Mexican', 'Chinese',\n",
       "       'American (New)', 'Breakfast & Brunch', 'Grocery', 'Bakeries',\n",
       "       'Specialty Food', 'Italian', 'Shopping', 'Ice Cream & Frozen Yogurt',\n",
       "       'Event Planning & Services', 'Desserts', 'Salad', 'Chicken Wings',\n",
       "       'Barbeque', 'Delis', 'Cafes', 'Asian Fusion', 'Korean', 'Sushi Bars',\n",
       "       'Beer', 'Wine & Spirits', 'Caterers', 'Sports Bars', 'Japanese',\n",
       "       'Food Trucks', 'Thai', 'Drugstores', 'Convenience Stores', 'Seafood',\n",
       "       'Steakhouses', 'Pubs', 'Juice Bars & Smoothies', 'Diners',\n",
       "       'Mediterranean', 'Indian', 'Hot Dogs', 'Arts & Entertainment',\n",
       "       'Bubble Tea', 'Donuts', 'Lounges', 'Venues & Event Spaces',\n",
       "       'Flowers & Gifts', 'Fashion', 'Food Delivery Services', 'Vegetarian',\n",
       "       'Ethnic Food', 'Tex-Mex', 'Breweries', 'Beauty & Spas',\n",
       "       'Department Stores', 'Automotive', 'Bagels'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have categories from column number 44\n",
    "business.columns[45:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_cols = [x for x in business.columns[45:]]\n",
    "\n",
    "dataset.fit_partial(items = business.business_id,\n",
    "                   item_features = tar_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fast Food',\n",
       " 'Nightlife',\n",
       " 'American (Traditional)',\n",
       " 'Bars',\n",
       " 'Pizza',\n",
       " 'Sandwiches',\n",
       " 'Burgers',\n",
       " 'Coffee & Tea',\n",
       " 'Mexican',\n",
       " 'Chinese',\n",
       " 'American (New)',\n",
       " 'Breakfast & Brunch',\n",
       " 'Grocery',\n",
       " 'Bakeries',\n",
       " 'Specialty Food',\n",
       " 'Italian',\n",
       " 'Shopping',\n",
       " 'Ice Cream & Frozen Yogurt',\n",
       " 'Event Planning & Services',\n",
       " 'Desserts',\n",
       " 'Salad',\n",
       " 'Chicken Wings',\n",
       " 'Barbeque',\n",
       " 'Delis',\n",
       " 'Cafes',\n",
       " 'Asian Fusion',\n",
       " 'Korean',\n",
       " 'Sushi Bars',\n",
       " 'Beer',\n",
       " 'Wine & Spirits',\n",
       " 'Caterers',\n",
       " 'Sports Bars',\n",
       " 'Japanese',\n",
       " 'Food Trucks',\n",
       " 'Thai',\n",
       " 'Drugstores',\n",
       " 'Convenience Stores',\n",
       " 'Seafood',\n",
       " 'Steakhouses',\n",
       " 'Pubs',\n",
       " 'Juice Bars & Smoothies',\n",
       " 'Diners',\n",
       " 'Mediterranean',\n",
       " 'Indian',\n",
       " 'Hot Dogs',\n",
       " 'Arts & Entertainment',\n",
       " 'Bubble Tea',\n",
       " 'Donuts',\n",
       " 'Lounges',\n",
       " 'Venues & Event Spaces',\n",
       " 'Flowers & Gifts',\n",
       " 'Fashion',\n",
       " 'Food Delivery Services',\n",
       " 'Vegetarian',\n",
       " 'Ethnic Food',\n",
       " 'Tex-Mex',\n",
       " 'Breweries',\n",
       " 'Beauty & Spas',\n",
       " 'Department Stores',\n",
       " 'Automotive',\n",
       " 'Bagels']"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'name', 'review_count', 'yelping_since', 'useful', 'funny',\n",
       "       'cool', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n",
       "       'compliment_more', 'compliment_profile', 'compliment_cute',\n",
       "       'compliment_list', 'compliment_note', 'compliment_plain',\n",
       "       'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
       "       'compliment_photos', 'is_elite', 'year', 'Shopping', 'Hot Dogs',\n",
       "       'Cafes', 'American (New)', 'Vegetarian', 'American (Traditional)',\n",
       "       'Beer', 'Delis', 'Grocery', 'Salad', 'Fast Food',\n",
       "       'Ice Cream & Frozen Yogurt', 'Donuts', 'Breakfast & Brunch',\n",
       "       'Chicken Wings', 'Burgers', 'Bakeries', 'Indian', 'Convenience Stores',\n",
       "       'Bubble Tea', 'Steakhouses', 'Thai', 'Mexican', 'Mediterranean',\n",
       "       'Food Delivery Services', 'Sushi Bars', 'Coffee & Tea',\n",
       "       'Event Planning & Services', 'Chinese', 'Sandwiches', 'Seafood',\n",
       "       'Specialty Food', 'Barbeque', 'Nightlife', 'Japanese', 'Desserts',\n",
       "       'Arts & Entertainment', 'Korean', 'Breweries', 'Food Trucks', 'Italian',\n",
       "       'Asian Fusion', 'Bagels', 'Juice Bars & Smoothies', 'Pizza'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cols = [x for x in users.drop(columns = ['user_id', 'name', 'yelping_since', 'elite', 'friends', 'fans', 'average_stars', 'compliment_hot',\n",
    "       'compliment_more', 'compliment_profile', 'compliment_cute',\n",
    "       'compliment_list', 'compliment_note', 'compliment_plain',\n",
    "       'compliment_cool', 'compliment_funny', 'compliment_writer',\n",
    "       'compliment_photos', 'year',]).columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.fit_partial(users = users.user_id,\n",
    "                    user_features = user_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lightfm.data.Dataset'>\n",
      "(12909, 909)\n",
      "(12859, 12909)\n",
      "(845, 909)\n",
      "(12859, 845)\n"
     ]
    }
   ],
   "source": [
    "# Check data\n",
    "print(type(dataset))\n",
    "print(dataset.model_dimensions())\n",
    "print(dataset.user_features_shape())\n",
    "print(dataset.item_features_shape())\n",
    "print(dataset.interactions_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9A1C1f0m4nQltQrOOTl-Kw', 0),\n",
       " ('VHsNB3pdGVcRgs6C3jt6Zg', 1),\n",
       " ('Ah4i15g8Ow_zphzcpulTxQ', 2),\n",
       " ('9MnbQg7kfb_WgxoV0hXKSQ', 3),\n",
       " ('t_yiQnxUDdPPCN2z4QyezA', 4),\n",
       " ('-fiUXzkxRfbHY9TKWwuptw', 5),\n",
       " ('NEVA0IYbawceL6kz5v5DAw', 6),\n",
       " ('RwMlwusAtxZc5a3ZYduulg', 7),\n",
       " ('ObNQVg_ohRVLex4ppmMC5w', 8),\n",
       " ('-Jhlh8Scjy669NdtCfKSSg', 9)]"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at item feature mapping\n",
    "a = dataset.mapping()[3]\n",
    "list(a.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build interaction\n",
    "(interactions, weights) = dataset.build_interactions([(x['user_id'], x['business_id'], \n",
    "                                                       x['stars']) for index,x in review.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<12859x845 sparse matrix of type '<class 'numpy.int32'>'\n",
      "\twith 34223 stored elements in COOrdinate format>\n"
     ]
    }
   ],
   "source": [
    "print(repr(interactions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum business review count\n",
      "6.723832440821209\n",
      "maximum user review count\n",
      "8.482394685873542\n",
      "<845x909 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 54080 stored elements in Compressed Sparse Row format>\n",
      "(845, 909)\n",
      "<12859x12909 sparse matrix of type '<class 'numpy.float32'>'\n",
      "\twith 655809 stored elements in Compressed Sparse Row format>\n",
      "(12859, 12909)\n"
     ]
    }
   ],
   "source": [
    "def build_dict(df,tar_cols,val_list):\n",
    "    rst = {}\n",
    "    for col in tar_cols:\n",
    "        rst[col] = df[col]\n",
    "    sum_val = sum(list(rst.values())) # get sum of all the tfidf values\n",
    "    \n",
    "    if(sum_val == 0):\n",
    "        return rst\n",
    "    else:\n",
    "        \n",
    "        w = (2-sum(val_list))/sum_val # weight for each tag to be able to sum to 1\n",
    "        for key,value in rst.items():\n",
    "            rst[key] = value * w\n",
    "    return rst\n",
    "\n",
    "# get max of each column to regularize value to [0,1]\n",
    "max_star = max(business.stars)\n",
    "max_b_review_count = max(business.review_count)\n",
    "print('maximum business review count')\n",
    "print(max_b_review_count)\n",
    "\n",
    "# give CF info weight 0.5, all other 0.5. Then in others, give (star, review count) 0.25 and tags 0.25\n",
    "item_features = dataset.build_item_features([(x['business_id'], \n",
    "                                              {'stars':0.5*x['stars']/max_star,\n",
    "                                               'review_count':0.5*x['review_count']/max_b_review_count,\n",
    "                                               **build_dict(x,tar_cols,[0.5*x['stars']/max_star,\n",
    "                                                           0.5*x['review_count']/max_b_review_count])})\n",
    "                                              for index,x in business.iterrows()])\n",
    "\n",
    "max_u_review_count = max(users.review_count)\n",
    "max_useful = max(users.useful)\n",
    "print('maximum user review count')\n",
    "print(max_u_review_count)\n",
    "user_features = dataset.build_user_features(((x['user_id'],\n",
    "                                             {'review_count':0.35*x['review_count']/max_u_review_count,'is_elite':0.35*int(x['is_elite']),\n",
    "                                              'useful':0.35*x['useful']/max_useful,\n",
    "                                             **build_dict(x,user_cols,[0.35*x['review_count']/max_u_review_count,\n",
    "                                                                            0.35*int(x['is_elite']),\n",
    "                                                                            0.35*x['useful']/max_useful])})\n",
    "                                           for index, x in users.iterrows()))\n",
    "\n",
    "print(repr(item_features))\n",
    "print(item_features.shape)\n",
    "\n",
    "print(repr(user_features))\n",
    "print(user_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 909)\n",
      "[array([0, 0, 0, 0, 0, 0, 0, 0]), array([  3, 845, 846, 849, 850, 851, 870, 887])]\n",
      "(1, 12909)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 0]), array([    3, 12859, 12860, 12869])]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# check features to see if weights make sense\n",
    "idx = 3\n",
    "tt = list(item_features[idx].nonzero())\n",
    "print(item_features[idx].shape)\n",
    "print(tt)\n",
    "\n",
    "uu = list(user_features[idx].nonzero())\n",
    "print(user_features[idx].shape)\n",
    "uu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0.0, 0.33333334, 0.07, 0.14999999, 0.16666667}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{0.0, 0.114341676, 0.23776683, 0.29328063, 0.35461086}]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tt = item_features[idx].todense()\n",
    "import pandas as pd\n",
    "tt = pd.DataFrame(tt)\n",
    "print(list(map(set,tt.values)))# feature weight sum to 1, so if two are nonzero, each take weight 0.5. \n",
    "\n",
    "uu = user_features[idx].todense()\n",
    "uu = pd.DataFrame(uu)\n",
    "list(map(set,uu.values))# feature weight sum to 1, so if two are nonzero, each take weight 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>899</th>\n",
       "      <th>900</th>\n",
       "      <th>901</th>\n",
       "      <th>902</th>\n",
       "      <th>903</th>\n",
       "      <th>904</th>\n",
       "      <th>905</th>\n",
       "      <th>906</th>\n",
       "      <th>907</th>\n",
       "      <th>908</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 909 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2         3    4    5    6    7    8    9    ...  899  900  901  \\\n",
       "0  0.0  0.0  0.0  0.333333  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "   902  903  904  905  906  907  908  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[1 rows x 909 columns]"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12899</th>\n",
       "      <th>12900</th>\n",
       "      <th>12901</th>\n",
       "      <th>12902</th>\n",
       "      <th>12903</th>\n",
       "      <th>12904</th>\n",
       "      <th>12905</th>\n",
       "      <th>12906</th>\n",
       "      <th>12907</th>\n",
       "      <th>12908</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.354611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 12909 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2         3      4      5      6      7      8      9      \\\n",
       "0    0.0    0.0    0.0  0.354611    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "   ...  12899  12900  12901  12902  12903  12904  12905  12906  12907  12908  \n",
       "0  ...    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[1 rows x 12909 columns]"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 12859 users and 845 items, with 6845 interactions in the test and 27378 interactions in the training set.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train-test split\n",
    "\n",
    "seed = 1001\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "train,test=random_train_test_split(interactions,test_percentage=0.2,random_state=np.random.RandomState(seed))\n",
    "\n",
    "print('The dataset has %s users and %s items, '\n",
    "      'with %s interactions in the test and %s interactions in the training set.'\n",
    "      % (train.shape[0], train.shape[1], test.getnnz(), train.getnnz()))\n",
    "\n",
    "train.multiply(test).nnz == 0 # make sure train and test are truly disjoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.48 s\n",
      "Collaborative filtering train AUC: 0.8142713\n",
      "Collaborative filtering test AUC: 0.805228\n",
      "Train precision: 0.0429\n",
      "Test precision: 0.0320\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from lightfm import LightFM\n",
    "\n",
    "# Set the number of threads; you can increase this if you have more physical cores available.\n",
    "\n",
    "## These parameters are obtained after tunning for specific loss through skopt in the bottom chunk\n",
    "NUM_THREADS = 25\n",
    "NUM_COMPONENTS = 43    \n",
    "NUM_EPOCHS = 18\n",
    "ITEM_ALPHA = 2.88752e-6\n",
    "learning_rate=0.06652\n",
    "k = 5 # for precision at k\n",
    "\n",
    "## Pure Collaborative Filtering models\n",
    "\n",
    "# Logistic loss\n",
    "model_1 = LightFM(loss='logistic',random_state=seed,\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "               no_components=NUM_COMPONENTS,\n",
    "               learning_rate=learning_rate)\n",
    "\n",
    "# time it.\n",
    "%time model_1 = model.fit(train,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "\n",
    "# Import the evaluation routines\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model, train, num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model, test,num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering test AUC: %s' % test_auc)\n",
    "\n",
    "from lightfm.evaluation import precision_at_k,recall_at_k\n",
    "\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model, train, k=k,num_threads=NUM_THREADS).mean())\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model, test,train_interactions=train, k=k,num_threads=NUM_THREADS).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.2 s\n",
      "Collaborative filtering train AUC: 0.85738117\n",
      "Collaborative filtering test AUC: 0.5631318\n",
      "Train precision: 0.1526\n",
      "Test precision: 0.0165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NUM_THREADS = 25\n",
    "NUM_COMPONENTS = 45    \n",
    "NUM_EPOCHS = 43\n",
    "ITEM_ALPHA = 1.3846e-6\n",
    "learning_rate=0.0161445\n",
    "\n",
    "# BPR loss\n",
    "model = LightFM(loss='bpr',random_state=seed,\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "               no_components=NUM_COMPONENTS,\n",
    "               learning_rate=learning_rate)\n",
    "\n",
    "# time it.\n",
    "%time model = model.fit(train,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model, train, num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model, test,num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering test AUC: %s' % test_auc)\n",
    "\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model, train, k=k,num_threads=NUM_THREADS).mean())\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model, test,train_interactions=train, k=k,num_threads=NUM_THREADS).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we can see that the loss function bpr is showing serious overfitting with drop in accuracy of the train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.95 s\n",
      "Collaborative filtering train AUC: 0.96783483\n",
      "Collaborative filtering test AUC: 0.8013681\n",
      "Train precision: 0.2008\n",
      "Test precision: 0.0294\n"
     ]
    }
   ],
   "source": [
    "NUM_THREADS = 25\n",
    "NUM_COMPONENTS = 21    \n",
    "NUM_EPOCHS = 16\n",
    "ITEM_ALPHA = 5.97967e-6\n",
    "learning_rate=0.033\n",
    "\n",
    "# Let's fit a WARP model\n",
    "model = LightFM(loss='warp',random_state=seed,\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "               no_components=NUM_COMPONENTS,\n",
    "               learning_rate=learning_rate)\n",
    "\n",
    "#  time it.\n",
    "%time model = model.fit(train,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model, train, num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model, test,num_threads=NUM_THREADS).mean()\n",
    "print('Collaborative filtering test AUC: %s' % test_auc)\n",
    "\n",
    "\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model, train, k=k,num_threads=NUM_THREADS).mean())\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model, test,train_interactions=train, k=k,num_threads=NUM_THREADS).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.7 s\n",
      " Hybrid train AUC: 0.7422592\n",
      "Hybrid test AUC: 0.73809147\n",
      "Train precision: 0.0148\n",
      "Test precision: 0.0097\n"
     ]
    }
   ],
   "source": [
    "NUM_THREADS = 25\n",
    "NUM_COMPONENTS = 30\n",
    "learning_rate = 0.0053\n",
    "NUM_EPOCHS = 6\n",
    "ITEM_ALPHA = 2.228e-5\n",
    "\n",
    "## Hybrid models\n",
    "\n",
    "#Combine user_feature and item feature\n",
    "\n",
    "#logistic loss\n",
    "model.iii = LightFM(loss='logistic',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                    random_state=seed,\n",
    "               no_components=NUM_COMPONENTS,learning_rate=learning_rate)\n",
    "\n",
    "# time it.\n",
    "%time model.iii = model.iii.fit(train,user_features=user_features,item_features=item_features,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model.iii, train,user_features=user_features,item_features=item_features, num_threads=NUM_THREADS).mean()\n",
    "print(' Hybrid train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model.iii, test,user_features=user_features,item_features=item_features,num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test AUC: %s' % test_auc)\n",
    "\n",
    "#precision @k\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model.iii, train,\n",
    "                                               item_features=item_features,user_features=user_features, k=k,\n",
    "                                               num_threads=NUM_THREADS).mean())\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model.iii, test,train_interactions=train,\n",
    "                                              item_features=item_features,user_features=user_features, k=k,\n",
    "                                             num_threads=NUM_THREADS).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 6s\n",
      "Hybrid train AUC: 0.93435514\n",
      "Hybrid test AUC: 0.8115924\n",
      "Train precision: 0.0502\n",
      "Test precision: 0.0263\n"
     ]
    }
   ],
   "source": [
    "NUM_THREADS = 25\n",
    "NUM_COMPONENTS = 77    \n",
    "NUM_EPOCHS = 80\n",
    "ITEM_ALPHA = 1.419e-6\n",
    "learning_rate=0.035\n",
    "\n",
    "\n",
    "# BPR\n",
    "model.iii = LightFM(loss='bpr',\n",
    "                item_alpha=ITEM_ALPHA,\n",
    "                     random_state=seed,\n",
    "               no_components=NUM_COMPONENTS,learning_rate=learning_rate)\n",
    "\n",
    "#  time it.\n",
    "%time model.iii = model.iii.fit(train,user_features=user_features,item_features=item_features,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model.iii, train,user_features=user_features,item_features=item_features, num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model.iii, test,user_features=user_features,item_features=item_features,num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test AUC: %s' % test_auc)\n",
    "\n",
    "#precision @k\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model.iii, train,\n",
    "                                               item_features=item_features,user_features=user_features, k=k,\n",
    "                                              num_threads=NUM_THREADS).mean())#0.41\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model.iii, test,train_interactions=train,\n",
    "                                              item_features=item_features,user_features=user_features, k=k,\n",
    "                                             num_threads=NUM_THREADS).mean())#0.17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 25s\n",
      "Hybrid train AUC: 0.9542297\n",
      "Hybrid test AUC: 0.8861522\n",
      "Train precision: 0.0957\n",
      "Test precision: 0.0578\n"
     ]
    }
   ],
   "source": [
    "NUM_THREADS = 50\n",
    "NUM_COMPONENTS = 42    \n",
    "NUM_EPOCHS = 30\n",
    "ITEM_ALPHA = 0.000256\n",
    "learning_rate=0.0529\n",
    "# WARP\n",
    "model.iii = LightFM(loss='warp',\n",
    "                item_alpha=ITEM_ALPHA, random_state=seed,\n",
    "               no_components=NUM_COMPONENTS,learning_rate=learning_rate,learning_schedule='adagrad')\n",
    "\n",
    "#time it.\n",
    "%time model.iii = model.iii.fit(train,user_features=user_features,item_features=item_features,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model.iii, train,user_features=user_features,item_features=item_features, num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model.iii, test,user_features=user_features,item_features=item_features,num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test AUC: %s' % test_auc)\n",
    "\n",
    "#precision @k\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model.iii, train,\n",
    "                                               item_features=item_features,user_features=user_features, k=k,\n",
    "                                              num_threads=NUM_THREADS).mean())\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model.iii, test,train_interactions=train,\n",
    "                                              item_features=item_features,user_features=user_features, k=k,\n",
    "                                             num_threads=NUM_THREADS).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 37s\n",
      "Hybrid train AUC: 0.9622207\n",
      "Hybrid test AUC: 0.8829761\n",
      "Train precision: 0.0981\n",
      "Test precision: 0.0577\n"
     ]
    }
   ],
   "source": [
    "NUM_THREADS = 50\n",
    "NUM_COMPONENTS = 42    \n",
    "NUM_EPOCHS = 30\n",
    "ITEM_ALPHA = 0.000256\n",
    "learning_rate=0.0529\n",
    "# WARP\n",
    "model.iii = LightFM(loss='warp-kos',\n",
    "                item_alpha=ITEM_ALPHA, random_state=seed,\n",
    "               no_components=NUM_COMPONENTS,learning_rate=learning_rate,learning_schedule='adagrad')\n",
    "\n",
    "#time it.\n",
    "%time model.iii = model.iii.fit(train,user_features=user_features,item_features=item_features,epochs=NUM_EPOCHS,num_threads=NUM_THREADS)\n",
    "\n",
    "\n",
    "\n",
    "# Compute and print the AUC score\n",
    "train_auc = auc_score(model.iii, train,user_features=user_features,item_features=item_features, num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid train AUC: %s' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model.iii, test,user_features=user_features,item_features=item_features,num_threads=NUM_THREADS).mean()\n",
    "print('Hybrid test AUC: %s' % test_auc)\n",
    "\n",
    "#precision @k\n",
    "print(\"Train precision: %.4f\" % precision_at_k(model.iii, train,\n",
    "                                               item_features=item_features,user_features=user_features, k=k,\n",
    "                                              num_threads=NUM_THREADS).mean())\n",
    "print(\"Test precision: %.4f\" % precision_at_k(model.iii, test,train_interactions=train,\n",
    "                                              item_features=item_features,user_features=user_features, k=k,\n",
    "                                             num_threads=NUM_THREADS).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def sample_train_recommendation(model, train, data_meta, user_ids, k, name, mapping, tag=None, user_features=None,\n",
    "                                item_features=None, num_threads=2):\n",
    "    n_users, n_items = train.shape\n",
    "\n",
    "    # =============================================================================\n",
    "    #     ranks = model.predict_rank(interactions,\n",
    "    #                                train_interactions=train_interactions,\n",
    "    #                                user_features=user_features,\n",
    "    #                                item_features=item_features,\n",
    "    #                                num_threads=num_threads,\n",
    "    #                                check_intersections=check_intersections,\n",
    "    #                                )\n",
    "    #\n",
    "    #     ranks.data = np.less(ranks.data, k, ranks.data)\n",
    "    #\n",
    "    #     precision = np.squeeze(np.array(ranks.sum(axis=1))) / k\n",
    "    #\n",
    "    #     if not preserve_rows:\n",
    "    #         precision = precision[test_interactions.getnnz(axis=1) > 0]\n",
    "    #\n",
    "    #     return precision\n",
    "    # =============================================================================\n",
    "\n",
    "    for user_id in user_ids:\n",
    "\n",
    "        t_idx = {value: key for key, value in mapping.items()}\n",
    "        u_idx = [x for x in train.tocsr()[user_id].indices]\n",
    "        known_positives = data_meta.loc[u_idx, name]  # may need change\n",
    "        if tag is not None:\n",
    "            known_tags = data_meta.loc[u_idx, tag]  # get item tags.\n",
    "\n",
    "        if (len(known_positives) < k):\n",
    "            print('not enough known positives, return max number')\n",
    "\n",
    "        scores = model.predict(user_id, np.arange(n_items), user_features=user_features, item_features=item_features,\n",
    "                               num_threads=num_threads)\n",
    "        i_idx = [x for x in np.argsort(-scores)]\n",
    "        top_items = data_meta.loc[i_idx, name]\n",
    "        if tag is not None:\n",
    "            top_tags = data_meta.loc[i_idx, tag]  # get item tags.\n",
    "\n",
    "        printmd(\"**User %s**\" % user_id)\n",
    "        printmd(\"**Known positives:**\")\n",
    "\n",
    "        if tag is not None:\n",
    "            for x in range(len(known_positives)):\n",
    "                print(\" %s | %s\" % (known_positives.values[x], known_tags.values[x]))\n",
    "        else:\n",
    "            for x in known_positives[:len(known_positives)]:\n",
    "                print(\"        %s\" % x)\n",
    "\n",
    "        printmd(\"**Recommended:**\")\n",
    "        cnt = 0\n",
    "        if tag is not None:\n",
    "            for x in range(k):\n",
    "                print(\" %s | %s\" % (top_items.values[x], top_tags.values[x]))\n",
    "                if (top_items.values[x] in known_positives.values):\n",
    "                    cnt += 1\n",
    "                    print('This one clicked')\n",
    "        else:\n",
    "            for x in top_items[:k]:\n",
    "                print(\"        %s\" % x)\n",
    "                if (x in known_positives.values):\n",
    "                    cnt += 1\n",
    "                    print('This one clicked')\n",
    "        #printmd('*cnt: *' + str(cnt))\n",
    "        printmd('*k_p: %s*'%str(len(known_positives)))\n",
    "        p_k = cnt / k\n",
    "        printmd('*precicion at k : %s*'%str(p_k))\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "def sample_test_recommendation(model, train, test, data_meta, user_ids, k, name, mapping, tag=None,\n",
    "                               train_interactions=None, user_features=None,\n",
    "                               item_features=None, num_threads=2):\n",
    "    n_users, n_items = test.shape\n",
    "\n",
    "    for user_id in user_ids:\n",
    "        \n",
    "        printmd(\"**User %s**\" % user_id)\n",
    "        \n",
    "        t_idx = {value: key for key, value in mapping.items()}\n",
    "        u_idx = [x for x in test.tocsr()[user_id].indices]\n",
    "\n",
    "        known_positives = data_meta.loc[u_idx, name]  # may need change\n",
    "\n",
    "        print('length of known_positives: ' + str(len(known_positives)))\n",
    "        if (len(known_positives) == 0):\n",
    "            sample_train_recommendation(model, train, data_meta, [user_id], k, name, mapping, tag, user_features,\n",
    "                                        item_features)\n",
    "            continue\n",
    "\n",
    "        elif (len(known_positives) < k):\n",
    "            print('not enough known positives, return max number')\n",
    "\n",
    "        if tag is not None:\n",
    "            known_tags = data_meta.loc[u_idx, tag]  # get item tags.\n",
    "\n",
    "        if (train_interactions is None):\n",
    "            scores = model.predict(user_id, np.arange(n_items), user_features=user_features,\n",
    "                                   item_features=item_features,\n",
    "                                   num_threads=num_threads)\n",
    "            i_idx = [x for x in np.argsort(-scores)]\n",
    "            top_items = data_meta.loc[i_idx, name]\n",
    "            if tag is not None:\n",
    "                top_tags = data_meta.loc[i_idx, tag]  # get item tags.\n",
    "\n",
    "        else:\n",
    "            item_ids = np.delete(np.arange(n_items), train.tocsr()[user_id].indices)\n",
    "            scores = model.predict(user_id, item_ids, user_features=user_features, item_features=item_features,\n",
    "                                   num_threads=num_threads)\n",
    "            i_idx = [x for x in np.argsort(-scores)]\n",
    "            top_items = data_meta.loc[i_idx, name]\n",
    "            if tag is not None:\n",
    "                top_tags = data_meta.loc[i_idx, tag]  # get item tags.\n",
    "\n",
    "        \n",
    "        printmd(\"**Known positives:**\")\n",
    "\n",
    "        if tag is not None:\n",
    "            for x in range(len(known_positives)):\n",
    "                print(\" %s | %s\" % (known_positives.values[x], known_tags.values[x]))\n",
    "        else:\n",
    "            for x in known_positives[:len(known_positives)]:\n",
    "                print(\"        %s\" % x)\n",
    "\n",
    "        printmd(\"**Recommended:**\")\n",
    "        cnt = 0\n",
    "        if tag is not None:\n",
    "            for x in range(k):\n",
    "                print(\" %s | %s\" % (top_items.values[x], top_tags.values[x]))\n",
    "                if (top_items.values[x] in known_positives.values):\n",
    "                    cnt += 1\n",
    "                    print('This one clicked')\n",
    "        else:\n",
    "            for x in top_items[:k]:\n",
    "                print(\"        %s\" % x)\n",
    "                if (x in known_positives.values):\n",
    "                    cnt += 1\n",
    "                    print('This one clicked')\n",
    "        #printmd('*cnt: *' + str(cnt))\n",
    "        printmd('*k_p: %s*'%str(len(known_positives)))\n",
    "        p_k = cnt / k\n",
    "        printmd('*precicion at k : %s*'%str(p_k))\n",
    "        print('----------------------------------------------------------------------')\n",
    "\n",
    "def get_user_index(test):\n",
    "    return scipy.sparse.find(test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough known positives, return max number\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**User 105**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Known positives:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Monical's Pizza | Italian, Pizza, Restaurants\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Recommended:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Monical's Pizza | Italian, Pizza, Restaurants\n",
      "This one clicked\n",
      " Siam Terrace | Thai, Sushi Bars, Restaurants\n",
      " Dunkin' | Food, Coffee & Tea, Bagels, Donuts\n",
      " Schnucks Savoy | Pharmacy, Bakeries, Health & Medical, Food, Grocery, Beer, Wine & Spirits\n",
      " V Picasso | Wine Bars, Event Planning & Services, Farmers Market, Bars, Food, Tapas Bars, Venues & Event Spaces, Nightlife, Breakfast & Brunch, Restaurants, American (New), Sandwiches, American (Traditional)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*k_p: 1*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*precicion at k : 0.2*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**User 62**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of known_positives: 4\n",
      "not enough known positives, return max number\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Known positives:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Bread Company | Bakeries, American (New), Modern European, Food, Restaurants, Sandwiches, Delis\n",
      " Sushi Ichiban | Sushi Bars, Japanese, Restaurants\n",
      " Dunkin' | Coffee & Tea, Donuts, Food\n",
      " Home of Gourmet Chinese & Thai Restaurant | Restaurants, Thai, Chinese\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Recommended:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " B Won | Korean, Restaurants\n",
      " Dunkin' | Donuts, Food, Coffee & Tea\n",
      "This one clicked\n",
      " Alexander's Steakhouse | Steakhouses, Seafood, Restaurants\n",
      " Scratch | Sandwiches, American (Traditional), Seafood, Restaurants\n",
      " Jimmy John's | Fast Food, Sandwiches, Delis, Pizza, Food Delivery Services, Food, Restaurants\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "*k_p: 4*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "*precicion at k : 0.2*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test corresponding recpmmendation\n",
    "\n",
    "sample_train_recommendation(model.iii,train,business,[105],5,'name',mapping=dataset.mapping()[2],tag='category',\n",
    "                              user_features = user_features,item_features=item_features)\n",
    "user_index=list(set(get_user_index(test)))\n",
    "sample_test_recommendation(model.iii,train,test,business,[user_index[51]],5,'name',mapping=dataset.mapping()[2],\n",
    "                              train_interactions=train,tag='category',user_features = user_features,item_features=item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar tags for Chinese: ['Seafood', 'Barbeque', 'Asian Fusion', 'Sushi Bars', 'Convenience Stores']\n",
      "Most similar tags for Bars: ['Nightlife', 'Breweries', 'Sports Bars', 'Lounges', 'Arts & Entertainment']\n",
      "Most similar tags for Ice Cream & Frozen Yogurt: ['Desserts', 'Juice Bars & Smoothies', 'Bubble Tea', 'Bakeries', 'Food Trucks']\n",
      "Most similar tags for Italian: ['Event Planning & Services', 'Pizza', 'Caterers', 'Salad', 'Sandwiches']\n"
     ]
    }
   ],
   "source": [
    "# get similar tags\n",
    "def get_similar_tags(model, tag_id,k):\n",
    "    # Define similarity as the cosine of the angle\n",
    "    # between the tag latent vectors\n",
    "\n",
    "    # Normalize the vectors to unit length\n",
    "    tag_embeddings = (model.item_embeddings[849:].T\n",
    "                      / np.linalg.norm(model.item_embeddings[849:], axis=1)).T\n",
    "\n",
    "    query_embedding = tag_embeddings[tag_id]\n",
    "    similarity = np.dot(tag_embeddings, query_embedding)\n",
    "    most_similar = np.argsort(-similarity)[1:k+1]\n",
    "\n",
    "    return most_similar\n",
    "tag_labels = list(dataset.mapping()[3].keys())[849:]\n",
    "#tag_labels\n",
    "\n",
    "target_ls = ['Chinese','Bars','Ice Cream & Frozen Yogurt','Italian']\n",
    "for tag in target_ls:\n",
    "   tag_id = tag_labels.index(tag)\n",
    "   print('Most similar tags for %s: %s' % (tag_labels[tag_id],\n",
    "                                           [tag_labels[x] for x in get_similar_tags(model.iii, tag_id,5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
